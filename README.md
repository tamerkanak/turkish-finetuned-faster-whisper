# ğŸš€ Turkish Fine-Tuned Faster-Whisper with LoRA

Bu proje, **Faster-Whisper** modelinin TÃ¼rkÃ§e ses verileri ile fine-tune edilmesini ve ardÄ±ndan CTranslate2 formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lerek hÄ±zlÄ± ve verimli bir ÅŸekilde inference yapÄ±lmasÄ±nÄ± amaÃ§lamaktadÄ±r. EÄŸitim ve inference sÃ¼reÃ§leri **modÃ¼ler bir yapÄ±da** ayrÄ± dosyalarda tasarlanmÄ±ÅŸtÄ±r.

---

## ğŸ—‚ï¸ Proje YapÄ±sÄ±

```text
turkish-finetuned-faster-whisper/
â”œâ”€â”€ training.py              # EÄŸitim aÅŸamasÄ±
â”œâ”€â”€ inference.py             # Ä°nference aÅŸamasÄ±
â”œâ”€â”€ README.md                # Proje dokÃ¼mantasyonu
â”œâ”€â”€ whisper_finetuned/       # Fine-tune edilmiÅŸ model ve tokenizer
â””â”€â”€ whisper_ct2_model/       # CTranslate2'ye dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmÃ¼ÅŸ model
â””â”€â”€ whisper_checkpoints/     # Modelin eÄŸitim sÄ±rasÄ±ndaki checkpointleri
```

---

## ğŸ¯ Proje Ã–zeti

* **Model:** Faster-Whisper (Whisper-Tiny)
* **Veri Seti:** Mozilla Common Voice 17.0 (TÃ¼rkÃ§e)
* **Fine-Tuning YÃ¶ntemi:** LoRA (Low-Rank Adaptation)
* **Epoch:** 35
* **Learning Rate:** 1e-5
* **Ã‡Ä±ktÄ±:** Her epoch sonunda eÄŸitim ve doÄŸrulama sonuÃ§larÄ± (Loss, WER)

---

## âš™ï¸ KullanÄ±lan Teknolojiler

* Python 3.10+
* Hugging Face Transformers
* Hugging Face Datasets
* Faster-Whisper
* CTranslate2
* LoRA (peft)

---

## ğŸ“¦ Kurulum

```bash
pip install evaluate jiwer datasets peft faster-whisper ctranslate2
```

---

## ğŸ‹ï¸ EÄŸitim AÅŸamasÄ±

### `training.ipynb`
ğŸ“Œ Alternatif olarak Google Colab Ã¼zerinden Ã§alÄ±ÅŸmak iÃ§in:
[ğŸ““ Colab'da EÄŸitim Notebook'u](https://colab.research.google.com/drive/19GAveEqQ7Ks9l4lkWRUOrGp3Tu0Rjgh8?usp=sharing)

Bu dosya ÅŸunlarÄ± yapar:

* Mozilla Common Voice veri setini indirir.
* LoRA ile Whisper Tiny modelini fine-tune eder.
* Her epoch'ta loss ve WER hesaplayÄ±p gÃ¶rselleÅŸtirir.
* Modeli hem Hugging Face formatÄ±nda hem de CTranslate2 formatÄ±nda kaydeder.
* Checkpoint sistemi ile eÄŸitim kesilse bile devam etmenizi saÄŸlar.

EÄŸitimden sonra model:

```text
/content/drive/My Drive/whisper_finetuned
```

klasÃ¶rÃ¼ne kaydedilir.

---

## ğŸ“ Ä°nference AÅŸamasÄ±

### `inference.ipynb`
ğŸ“Œ Alternatif olarak Google Colab Ã¼zerinden Ã§alÄ±ÅŸmak iÃ§in:

[ğŸ““ Colab'da Ä°nference Notebook'u](https://colab.research.google.com/drive/1zsoZUOCqH8IcjZH_5HArb2NK2b6Y-T9T?usp=sharing)

Bu dosya ÅŸunlarÄ± yapar:

* EÄŸitilmiÅŸ CTranslate2 modelini yÃ¼kler.
* Belirlenen ses dosyalarÄ± iÃ§in TÃ¼rkÃ§e transkripsiyon yapar.
* SonuÃ§larÄ± terminalde detaylÄ± ÅŸekilde gÃ¶sterir.

---

## ğŸš€ Model DÃ¶nÃ¼ÅŸtÃ¼rme

EÄŸitim sonrasÄ± model ÅŸu komutla CTranslate2 formatÄ±na dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lÃ¼r:

```bash
ct2-transformers-converter --model "/content/drive/My Drive/whisper_finetuned" \
                           --output_dir "/content/drive/My Drive/whisper_ct2_model" \
                           --quantization float16 --force
```

> âœ… Bu dÃ¶nÃ¼ÅŸÃ¼m sonrasÄ± modeli `faster-whisper` kÃ¼tÃ¼phanesi ile kullanabilirsiniz.

---

## ğŸ“Š Ã‡Ä±ktÄ± Ã–rnekleri

Transkripsiyon sÄ±rasÄ±nda Ã¶rnek Ã§Ä±ktÄ±:

```text
Transcription Results:
Detected language: tr (probability: 1.00)
Transcription:
[0.00s -> 16.00s] Ne mutlu, tÃ¼rkÃ¼m diyene.
```

---

## ğŸ—’ï¸ Notlar

* GPU bellek verimliliÄŸi iÃ§in LoRA kullanÄ±lmÄ±ÅŸtÄ±r.
* Fine-tune iÅŸlemi dÃ¼ÅŸÃ¼k kaynaklÄ± makinelerde optimize edilmiÅŸtir.
* CTranslate2 dÃ¶nÃ¼ÅŸÃ¼mÃ¼ inference hÄ±zÄ±nÄ± ciddi oranda artÄ±rÄ±r.
* GitHub'da .ipynb dosyalarÄ±nÄ±n Ã§Ä±ktÄ± hÃ¼creleri bazen gÃ¶rÃ¼nmeyebilir. Ã‡Ä±ktÄ±larÄ± doÄŸrudan gÃ¶rmek isterseniz yukarÄ±da paylaÅŸÄ±lan Colab linklerini kullanabilirsiniz.

---

## ğŸ”— Kaynaklar

* [Faster-Whisper GitHub](https://github.com/SYSTRAN/faster-whisper)
* [Mozilla Common Voice Dataset](https://huggingface.co/datasets/mozilla-foundation/common_voice_17_0)
* [Hugging Face Whisper Modeli](https://huggingface.co/openai/whisper-tiny)

---

## ğŸ“Œ Ä°letiÅŸim

Herhangi bir soru ya da katkÄ± iÃ§in bana ulaÅŸabilirsiniz.

> âœ‰ï¸ \[tamerkanak75@gmail.com]
